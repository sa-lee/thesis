[
["index.html", "Fluent statistical computing interfaces for biological data analysis Welcome", " Fluent statistical computing interfaces for biological data analysis Stuart Lee Welcome This is the website for my PhD thesis at Monash University (Australia), titled “Fluent statistical computing interfaces for biological data analysis”. It is a work in progress. "],
["abstract.html", "Abstract", " Abstract "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements "],
["preface.html", "Preface", " Preface "],
["1-ch-intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction Exploratory data analysis (EDA) is a vital element of the modern statistical workflow - it is an analyst’s first pass at understanding their data; revealing all it’s messes and uncovering hidden insights (Tukey 1977; Grolemund and Wickham 2017). It is an iterative process involving computation and visualization, leading to new hypotheses that can be tested and formalised using statistical modelling. As datasets grow in complexity and become increasingly heterogeneous and multidimensional, the use of EDA becomes vital to ensure the integrity and quality of analysis outputs. This is certainly true in high-throughput biological data science, where constraints on computation time and memory, in addition to the analyst’s time, results in EDA becoming increasingly difficult and neglected. This thesis develops tools and frameworks to perform EDA as part of the biological data science workflow… "],
["1-1-a-grammar-for-genomic-data-analysis.html", "1.1 A grammar for genomic data analysis", " 1.1 A grammar for genomic data analysis The approach taken by the suite of software packages collectively known as the tidyverse is an attempt to formalise aspects of the EDA process in the R programming language under a single semantic known as tidy data (R Core Team 2019a; Wickham et al. 2019; Wickham 2014). Simply put, a tidy data set is a rectangular table where each row of the table corresponds to an observation, each column corresponds to a variable and each cell a value. There is a surprisingly large amount of utility that can be achieved with this definition. By having each column representing a variable, variables in the data can be mapped to graphical aesthetics of plots. This paradigm enables the grammar of graphics as implemented by ggplot2 (Wickham, Hadley 2016; Wilkinson 2005). User interfaces as implemented by tidyverse, and in particular the dplyr package, are fluent; they form a domain specific language (DSL) that gives users a mental model for performing and composing common data transformation tasks (Wickham et al. 2017; Fowler, n.d.). It is unclear whether the fluent interfaces as implemented using the tidy data framework can be more generally applied and useful in fields such as high-throughput biology where domain specific semantics are required. This is particularly true in the Bioconductor ecosystem, where much thought has gone into the design of data structures that enable interoperability between different tools, biological assays and analysis goals (Huber et al. 2015a). Chapter 2 shows that the tidy data semantic is applicable to range-based genomics data and develops a fluent interface to transforming it called plyranges. The software provides a framework to an assist an analyst to compose queries on genomics datasets. This chapter has been published in Genome Biology (Lee, Cook, and Lawrence 2019). "],
["1-2-integration-and-representation-of-genomic-data-structures.html", "1.2 Integration and representation of genomic data structures", " 1.2 Integration and representation of genomic data structures talking about chapters 3 and 4 here having the grammar doesn’t impair interoperability, in fact we show it can be used to glue two different approaches the long form representation can be taken even further… recasting EDA problems as search, we want to find regions measured along the genome that display a characteristic of interest, and then link that to raw data plots. "],
["1-3-visual-analytics-for-dimension-reduction-workflows.html", "1.3 Visual analytics for dimension reduction workflows", " 1.3 Visual analytics for dimension reduction workflows talking about chapter 5 taking a step back in the last chapter, have been focused on data wrangling but am now looking the integration of vis and models focussed on a common part of EDA as applied to transcriptomics, dimension reduction workflows especially non-linear dr as that has gotten extremely popular combining with the tour, we can grasp the trade-off between global and local views of the data user interaction provides ability to dissect the quality of a DR, and perform cluster oriented tasks "],
["2-ch-plyranges.html", "Chapter 2 plyranges: a grammar of data transformation for genomics", " Chapter 2 plyranges: a grammar of data transformation for genomics There is a cognitive load placed on users in learning a data abstraction from the Bioconductor project and understanding its appropriate use. Users must navigate these abstractions to perform a genomic analysis task, when a single data abstraction, a GRanges object will suffice. By recognizing that the GRanges class follows ‘tidy’ data principles, we create a grammar of genomic data transformation, defining verbs for performing actions on and between genomic interval data and providing a way of performing common data analysis tasks through a coherent interface to existing Bioconductor infrastructure. We implement this grammar as a Bioconductor/R package called plyranges. "],
["2-1-background.html", "2.1 Background", " 2.1 Background High-throughput genomics promises to unlock new disease therapies, and strengthen our knowledge of basic biology. To deliver on those promises, scientists must derive a stream of knowledge from a deluge of data. Genomic data is challenging in both scale and complexity. Innovations in sequencing technology often outstrip our capacity to process the output. Beyond their common association with genomic coordinates, genomic data are heterogeneous, consisting of raw sequence read alignments, genomic feature annotations like genes and exons, and summaries like coverage vectors, ChIP-seq peak calls, variant calls, and per-feature read counts. Genomic scientists need software tools to wrangle the different types of data, process the data at scale, test hypotheses, and generate new ones, all while focusing on the biology, not the computation. For the tool developer, the challenge is to define ways to model and operate on the data that align with the mental model of scientists, and to provide an implementation that scales with their ambition. Several domain specific languages (DSLs) enable scientists to process and reason about heterogeneous genomics data by expressing common operations, such as range manipulation and overlap-based joins, using the vocabulary of genomics. Their implementations either delegate computations to a database, or operate over collections of files in standard formats like BED. An example of the former is the Genome Query Language (GQL) and its distributed implementation GenAp which use a SQL-like syntax for fast retrieval of information of unprocessed sequencing data (Kozanitis, Christos et al. 2014; Kozanitis and Patterson 2016). Similarly, the Genometric Query Language (GMQL) implements a DSL for combining genomic datasets (Kaitoua, A et al. 2017). The command line application BEDtools develops an extensive algebra for performing arithmetic between two or more sets of genomic regions (Quinlan and Hall 2010). All of the aforementioned DSLs are designed to be evaluated either at the command line or embedded in scripts for batch processing. They exist in a sparse ecosystem, mostly consisting of UNIX and database tools that lack biological semantics and operate at the level of files and database tables. The Bioconductor/R packages IRanges and GenomicRanges (R Core Team 2018; Lawrence et al. 2013a; Huber et al. 2015a) define a DSL for analyzing genomics data with R, an interactive data analysis environment that encourages reproducibility and provides high-level abstractions for manipulating, modelling and plotting data, through state of the art methods in statistical computing. The packages define object-oriented (OO) abstractions for representing genomic data and enable interoperability by allowing users and developers to use these abstractions in their own code and packages. Other genomic DSLs that are embedded in programming languages include pybedtools and valr (Dale, Pedersen, and Quinlan 2011; Riemondy et al. 2017), however these packages lack the interoperability provided by the aforementioned Bioconductor packages and are not easily extended. The Bioconductor infrastructure models the genomic data and operations from the perspective of the power user, one who understands and wants to take advantage of the subtle differences in data types. This design has enabled the development of sophisticated tools, as evidenced by the hundreds of packages depending on the framework. Unfortunately, the myriad of data structures have overlapping purposes and important but obscure differences in behavior that often confuse the typical end user. Recently, there has been a concerted, community effort to standardize R data structures and workflows around the notion of tidy data (Wickham 2014). A tidy dataset is defined as a tabular data structure that has observations as rows and columns as variables, and all measurements pertain to a single observational unit. The tidy data pattern is useful because it allows us to see how the data relate to the design of an experiment and the variables measured. The dplyr package (Wickham et al. 2017) defines an application programming interface (API) that maps notions from the general relational algebra to verbs that act on tidy data. These verbs can be composed together on one or more tidy datasets with the pipe operator from the magrittr package (Bache and Wickham 2014). Taken together these features enable a user to write human readable analysis workflows. We have created a genomic DSL called plyranges that reformulates notions from existing genomic algebras and embeds them in R as a genomic extension of dplyr. By analogy, plyranges is to the genomic algebra, as dplyr is to the relational algebra. The plyranges Bioconductor package implements the language on top of a key subset of Bioconductor data structures and thus fully integrates with the Bioconductor framework, gaining access to its scalable data representations and sophisticated statistical methods. "],
["2-2-results.html", "2.2 Results", " 2.2 Results 2.2.1 Genomic Relational Algebra 2.2.1.1 Data Model Figure 2.1: An illustration of the GRanges data model for a sample from an RNA-seq experiment. The core components of the data model include a seqname column (representing the chromosome), a ranges column which consists of start and end coordinates for a genomic region, and a strand identifier (either positive, negative, or unstranded). Metadata are included as columns to the right of the dotted line as annotations (gene_id) or range level covariates (score). The plyranges DSL is built on the core Bioconductor data structure GRanges, which is a constrained table, with fixed columns for the chromosome, start and end coordinates, and the strand, along with an arbitrary set of additional columns, consisting of measurements or metadata specific to the data type or experiment (figure 2.1). GRanges balances flexibility with formal constraints, so that it is applicable to virtually any genomic workflow, while also being semantically rich enough to support high-level operations on genomic ranges. As a core data structure, GRanges enables interoperability between plyranges and the rest of Bioconductor. Adhering to a single data structure simplifies the API and makes it easier to learn and understand, in part because operations become endomorphic, i.e., they return the same type as their input. GRanges follow the intuitive tidy data pattern: it is a rectangular table corresponding to a single biological context. Each row contains a single observation and each column is a variable describing the observations. GRanges specializes the tidy pattern in that the observations always pertain to some genomic feature, but it largely remains compatible with the general relational operations defined by dplyr. Thus, we define our algebra as an extension of the dplyr algebra, and borrow its syntax conventions and design principles. 2.2.1.2 Algebraic operations The plyranges DSL defines an expressive algebra for performing genomic operations with and between GRanges objects (see table ). The grammar includes several classes of operation that cover most use cases in genomics data analysis. There are range arithmetic operators, such as for resizing ranges or finding their intersection, and operators for merging, filtering and aggregating by range-specific notions like overlap and proximity. Arithmetic operations transform range coordinates, as defined by their start, end and width. The three dimensions are mutually dependent and partially redundant, so direct manipulation of them is problematic. For example, changing the width column needs to change either the start, end or both to preserve integrity of the object. We introduce the anchor modifier to disambiguate these adjustments. Supported anchor points include the start, end and midpoint, as well as the 3’ and 5’ ends for strand-directed ranges. For example, if we anchor the start, then setting the width will adjust the end while leaving the start stationary. The algebra also defines conveniences for relative coordinate adjustments: shift (unanchored adjustment to both start and end) and stretch (anchored adjustment of width). We can perform any relative adjustment by some combination of those two operations. The stretch operation requires an anchor and assumes the midpoint by default. Since shift is unanchored, the user specifies a suffix for indicating the direction: left/right or, for stranded features, upstream/downstream. For example, shift_right shifts a range to the right. The flank operation generates new ranges that are adjacent to existing ones. This is useful, for example, when generating upstream promoter regions for genes. Analogous to shift, a suffix indicates the side of the input range to flank. As with other genomic grammars, we define set operations that treat ranges as sets of integers, including intersect, union, difference, and complement. There are two sets of these: parallel and merging. For example, the parallel intersection (x %intersect% y) finds the intersecting range between xi and yi for i in 1…n, where n is the length of both x and y. In contrast, the merging intersection (intersect_ranges(x, y)) returns a new set of disjoint ranges representing wherever there was overlap between a range in x and a range in y. Finding the parallel union will fail when two ranges have a gap, so we introduce a span operator that takes the union while filling any gap. The complement operation is unique in that it is unary. It finds the regions not covered by any of the ranges in a single set. Closely related is the between parallel operation, which finds the gap separating xi and yi. The binary operations are callable from within arithmetic, restriction and aggregation expressions. Figure 2.2: Illustration of the three overlap join operators. Each join takes two GRanges objects, and as input. A ‘Hits’ object for the join is computed which consists of two components. The first component contains the indices of the ranges in that have been overlapped (the rectangles of that cross the orange lines). The second component consists of the indices of the ranges in that overlap the ranges in . In this case a range in overlaps the ranges in three times, so the index is repeated three times. The resulting ‘Hits’ object is used to modify by where it was ‘hit’ by and merge all metadata columns from and based on the indices contained in the ‘Hits’ object. This procedure is applied generally in the DSL for both overlap and nearest neighbor operations. The join semantics alter what is returned: : for an join the ranges that are overlapped by are returned. The returned ranges also include the metadata from the range that overlapped the three ranges. An join is identical to an inner join except that the intersection is taken between the overlapped ranges and the ranges. For the join all ranges are returned regardless of whether they are overlapped by . In this case the third range (rectangle with the asterisk next to it) of the join would have missing values on metadata columns that came from . To support merging, our algebra recasts finding overlaps or nearest neighbors between two genomic regions as variants of the relational join operator. A join acts on two GRanges objects: x and y. The join operator is relational in the sense that metadata from the x and y ranges are retained in the joined range. All join operators in the plyranges DSL generate a set of hits based on overlap or proximity of ranges and use those hits to merge the two datasets in different ways. There are four supported matching algorithms: overlap, nearest, precede, and follow (figure 2.2). We can further restrict the matching by whether the query is completely within the subject, and adding the directed suffix ensures that matching ranges have the same direction (strand). For merging based on the hits, we have three modes: inner, intersect and left. The inner overlap join is similar to the conventional inner join in that there is a row in the result for every match. A major difference is that the matching is not by identity, so we have to choose one of the ranges from each pair. We always choose the left range. The intersect join uses the intersection instead of the left range. Finally, the overlap left join is akin to left outer join in Codd’s relational algebra: it performs an overlap inner join but also returns all x ranges that are not hit by the y ranges. Figure 2.3: Idiomatic code examples for () and () illustrating an overlap and aggregate operation that returns the same result. In each example, we have two BED files consisting of SNPs that are genome-wide association study (GWAS) hits and reference exons. Each code block counts for each SNP the number of distinct exons it overlaps. The code achieves this with an overlap join followed by partitioning and aggregation. Strand is ignored by default here. The code achieves this using the ‘Hits’ and ‘List’ classes and their methods. Since the GRanges object is a tabular data structure, our grammar includes operators to filter, sort and aggregate by columns in a GRanges. These operations can be performed over partitions formed using the group_by modifier. Together with our algebra for arithmetic and merging, these operations conform to the semantics and syntax of the dplyr grammar. Consequently, plyranges code is generally more compact than the equivalent GenomicRanges code (figure 2.3). 2.2.2 Developing workflows with plyranges Here we provide illustrative examples of using the plyranges DSL to show how our grammar could be integrated into genomic data workflows. As we construct the workflows we show the data output intermittently to assist the reader in understanding the pipeline steps. The workflows highlight how interoperability with existing Bioconductor infrastructure, enables easy access to public datasets and methods for analysis and visualization. 2.2.2.1 Peak Finding In the workflow of ChIP-seq data analysis, we are interested in finding peaks from islands of coverage over chromosome. Here we will use plyranges to call peaks from islands of coverage above 8 then plot the region surrounding the tallest peak. Using plyranges and the the Bioconductor package AnnotationHub (Morgan 2017) we can download and read BigWig files from ChIP-Seq experiments from the Human Epigenome Roadmap project (Roadmap Epigenomics Consortium et al. 2015). Here we analyse a BigWig file corresponding to H3 lysine 27 trimethylation (H3K27Me3) of primary T CD8+ memory cells from peripheral blood, focussing on coverage islands over chromosome 10. First, we extract the genome information from the BigWig file and filter to get the range for chromosome 10. This range will be used as a filter when reading the file. library(plyranges) chr10_ranges &lt;- bw_file %&gt;% get_genome_info() %&gt;% filter(seqnames == &quot;chr10&quot;) Then we read the BigWig file only extracting scores if they overlap chromosome 10. We also add the genome build information to the resulting ranges. This book-keeping is good practice as it ensures the integrity of any downstream operations such as finding overlaps. chr10_scores &lt;- bw_file %&gt;% read_bigwig(overlap_ranges = chr10_ranges) %&gt;% set_genome_info(genome = &quot;hg19&quot;) chr10_scores #&gt; GRanges object with 5789841 ranges and 1 metadata column: #&gt; seqnames ranges strand | score #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;numeric&gt; #&gt; [1] chr10 1-60602 * | 0.0422799997031689 #&gt; [2] chr10 60603-60781 * | 0.163240000605583 #&gt; [3] chr10 60782-60816 * | 0.372139990329742 #&gt; [4] chr10 60817-60995 * | 0.163240000605583 #&gt; [5] chr10 60996-61625 * | 0.0422799997031689 #&gt; ... ... ... ... . ... #&gt; [5789837] chr10 135524723-135524734 * | 0.144319996237755 #&gt; [5789838] chr10 135524735-135524775 * | 0.250230014324188 #&gt; [5789839] chr10 135524776-135524784 * | 0.427789986133575 #&gt; [5789840] chr10 135524785-135524806 * | 0.730019986629486 #&gt; [5789841] chr10 135524807-135524837 * | 1.03103005886078 #&gt; ------- #&gt; seqinfo: 25 sequences from hg19 genome We then filter for regions with a coverage score greater than 8, and following this reduce individual runs to ranges representing the islands of coverage. This is achieved with the reduce_ranges() function, which allows a summary to be computed over each island: in this case we take the maximum of the scores to find the coverage peaks over chromosome 10. all_peaks &lt;- chr10_scores %&gt;% filter(score &gt; 8) %&gt;% reduce_ranges(score = max(score)) all_peaks #&gt; GRanges object with 1085 ranges and 1 metadata column: #&gt; seqnames ranges strand | score #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;numeric&gt; #&gt; [1] chr10 1299144-1299370 * | 13.2264003753662 #&gt; [2] chr10 1778600-1778616 * | 8.20512008666992 #&gt; [3] chr10 4613068-4613078 * | 8.76027011871338 #&gt; [4] chr10 4613081-4613084 * | 8.43659973144531 #&gt; [5] chr10 4613086 * | 8.11507987976074 #&gt; ... ... ... ... . ... #&gt; [1081] chr10 135344482-135344488 * | 9.23237991333008 #&gt; [1082] chr10 135344558-135344661 * | 11.843409538269 #&gt; [1083] chr10 135344663-135344665 * | 8.26965999603271 #&gt; [1084] chr10 135344670-135344674 * | 8.26965999603271 #&gt; [1085] chr10 135345440-135345441 * | 8.26965999603271 #&gt; ------- #&gt; seqinfo: 25 sequences from hg19 genome Returning to the GRanges object containing normalized coverage scores, we filter to find the coordinates of the peak containing the maximum coverage score. We can then find a 5000 nt region centered around the maximum position by anchoring and modifying the width. Finally, the overlap inner join is used to restrict the chromosome 10 coverage islands, to the islands that are contained in the 5000nt region that surrounds the max peak (figure 2.4). #&gt; GRanges object with 890 ranges and 2 metadata columns: #&gt; seqnames ranges strand | score.x score.y #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;numeric&gt; &lt;numeric&gt; #&gt; [1] chr10 21805891-21805988 * | 0.0206599999219179 29.9573001861572 #&gt; [2] chr10 21805989-21806000 * | 0.0211200006306171 29.9573001861572 #&gt; [3] chr10 21806001-21806044 * | 0.022069999948144 29.9573001861572 #&gt; [4] chr10 21806045-21806049 * | 0.0215900000184774 29.9573001861572 #&gt; [5] chr10 21806050-21806081 * | 0.0211200006306171 29.9573001861572 #&gt; ... ... ... ... . ... ... #&gt; [886] chr10 21810878 * | 5.24951982498169 29.9573001861572 #&gt; [887] chr10 21810879 * | 5.83534002304077 29.9573001861572 #&gt; [888] chr10 21810880-21810884 * | 6.44267988204956 29.9573001861572 #&gt; [889] chr10 21810885-21810895 * | 7.07054996490479 29.9573001861572 #&gt; [890] chr10 21810896-21810911 * | 6.44267988204956 29.9573001861572 #&gt; ------- #&gt; seqinfo: 25 sequences from hg19 genome Figure 2.4: The final result of the plyranges operations to find a 5000nt region surrounding the peak of normalised coverage scores over chromosome 10, displayed as a density plot. 2.2.2.2 Computing Windowed Statistics Another common operation in genomics data analysis is to compute data summaries over genomic windows. In plyranges this can be achieved via the group_by_overlaps() operator. We bin and count and find the average GC content of reads from a H3K27Me3 ChIP-seq experiment by the Human Epigenome Roadmap Consortium. We can directly obtain the genome information from the header of the BAM file: in this case the reads were aligned to the hg19 genome build and there are no reads overlapping the mitochondrial genome. bam &lt;- read_bam(h1_bam_sorted, index = h1_bam_sorted_index) locations &lt;- bam %&gt;% get_genome_info() Next we only read in alignments that overlap the genomic locations we are interested in and select the query sequence. Note that the reading of the BAM file is deferred: only alignments that pass the filter are loaded into memory. We can add another column representing the GC proportion for each alignment using the letterFrequency() function from the Biostrings package (Pagès et al. 2018). After computing the GC proportion as the score column, we drop all other columns in the GRanges object. alignments &lt;- bam %&gt;% filter_by_overlaps(locations) %&gt;% select(seq) %&gt;% mutate( score = as.numeric(letterFrequency(seq, &quot;GC&quot;, as.prob = TRUE)) ) %&gt;% select(score) alignments #&gt; GRanges object with 8275595 ranges and 1 metadata column: #&gt; seqnames ranges strand | score #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;numeric&gt; #&gt; [1] chr10 50044-50119 - | 0.276315789473684 #&gt; [2] chr10 50050-50119 + | 0.25 #&gt; [3] chr10 50141-50213 - | 0.447368421052632 #&gt; [4] chr10 50203-50278 + | 0.263157894736842 #&gt; [5] chr10 50616-50690 + | 0.276315789473684 #&gt; ... ... ... ... . ... #&gt; [8275591] chrY 57772745-57772805 - | 0.513157894736842 #&gt; [8275592] chrY 57772751-57772800 + | 0.526315789473684 #&gt; [8275593] chrY 57772767-57772820 + | 0.565789473684211 #&gt; [8275594] chrY 57772812-57772845 + | 0.25 #&gt; [8275595] chrY 57772858-57772912 + | 0.592105263157895 #&gt; ------- #&gt; seqinfo: 24 sequences from an unspecified genome Finally, we create 10000nt tiles over the genome and compute the number of reads and average GC content over all reads that fall within each tile using an overlap join and merging endpoints. bins &lt;- locations %&gt;% tile_ranges(width = 10000L) alignments_summary &lt;- bins %&gt;% join_overlap_inner(alignments) %&gt;% disjoin_ranges(n = n(), avg_gc = mean(score)) alignments_summary #&gt; GRanges object with 286030 ranges and 2 metadata columns: #&gt; seqnames ranges strand | n avg_gc #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;integer&gt; &lt;numeric&gt; #&gt; [1] chr10 49999-59997 * | 88 0.369019138755981 #&gt; [2] chr10 59998-69997 * | 65 0.434210526315789 #&gt; [3] chr10 69998-79996 * | 56 0.386513157894737 #&gt; [4] chr10 79997-89996 * | 71 0.51297257227576 #&gt; [5] chr10 89997-99996 * | 64 0.387746710526316 #&gt; ... ... ... ... . ... ... #&gt; [286026] chrY 57722961-57732958 * | 36 0.468201754385965 #&gt; [286027] chrY 57732959-57742957 * | 38 0.469529085872576 #&gt; [286028] chrY 57742958-57752956 * | 38 0.542936288088643 #&gt; [286029] chrY 57752957-57762955 * | 42 0.510651629072682 #&gt; [286030] chrY 57762956-57772954 * | 504 0.526942355889723 #&gt; ------- #&gt; seqinfo: 24 sequences from an unspecified genome; no seqlengths 2.2.2.3 Quality Control Metrics We have created a GRanges object from genotyping performed on the H1 cell line, consisting of approximately two million single nucleotide polymorphisms (SNPs) and short insertion/deletions (indels). The GRanges object consists of 7 columns, relating to the alleles of a SNP or indel, the B-allele frequency, log relative intensity of the probes, GC content score over a probe, and the name of the probe. We can use this information to compute the transition-transversion ratio, a quality control metric, within each chromosome in GRanges object. First we filter out the indels and mitochondrial variants. Then we create a logical vector corresponding to whether there is a transition event. h1_snp_array &lt;- h1_snp_array %&gt;% filter(!(ref %in% c(&quot;I&quot;, &quot;D&quot;)), seqnames != &quot;M&quot;) %&gt;% mutate(transition = (ref %in% c(&quot;A&quot;, &quot;G&quot;) &amp; alt %in% c(&quot;G&quot;,&quot;A&quot;))| (ref %in% c(&quot;C&quot;,&quot;T&quot;) &amp; alt %in% c(&quot;T&quot;, &quot;C&quot;))) We then compute the transition-transversion ratio over each chromosome using group_by() in combination with summarize() (figure 2.5). ti_tv_results &lt;- h1_snp_array %&gt;% group_by(seqnames) %&gt;% summarize(n_snps = n(), ti_tv = sum(transition) / sum(!transition)) ti_tv_results #&gt; DataFrame with 24 rows and 3 columns #&gt; seqnames n_snps ti_tv #&gt; &lt;factor&gt; &lt;integer&gt; &lt;numeric&gt; #&gt; 1 Y 2226 1.4381161007667 #&gt; 2 6 154246 3.32013219807305 #&gt; 3 13 83736 3.40669403220714 #&gt; 4 10 120035 3.49400973418195 #&gt; 5 4 153243 3.29528828096533 #&gt; ... ... ... ... #&gt; 20 16 77538 3.19827819589583 #&gt; 21 12 113208 3.47851887016378 #&gt; 22 20 57073 3.7121036988111 #&gt; 23 21 32349 3.50480434479877 #&gt; 24 X 55495 3.58219800181653 Figure 2.5: The final result of computing quality control metrics over the SNP array data with plyranges, displayed as a dot plot. Chromosomes are ordered by their estimated transition-transversion ratio. A white reference line is drawn at the expected ratio for a human exome.\" "],
["2-3-discussion.html", "2.3 Discussion", " 2.3 Discussion The design of plyranges adheres to well understood principles of language and API design: cognitive consistency, cohesion, endomorphism and expressiveness (T R G Green AND Petre 1996). To varying degrees, these principles also underlie the design of dplyr and the Bioconductor infrastructure. We have aimed for plyranges to have a simple and direct mapping to the user’s cognitive model, i.e., how the user thinks about the data. This requires careful selection of the level of abstraction so that the user can express workflows in the language of genomics. This motivates the adoption of the tidy GRanges object as our central data structure. The basic data.frame and dplyr tibble lack any notion of genomic ranges and so could not easily support our genomic grammar, with its specific verbs for range-oriented data manipulation. Another example of cognitive consistency is how plyranges is insensitive to direction/strand by default when, e.g., detecting overlaps. GenomicRanges has the opposite behavior. We believe that defaulting to purely spatial overlap is most intuitive to most users. To further enable cognitive consistency, plyranges functions are cohesive. A function is defined to be cohesive if it performs a singular task without producing any side-effects. Singular tasks can always be broken down further at lower levels of abstraction. For example, to resize a range, the user needs to specify which position (start, end, midpoint) should be invariant over the transformation. The resize() function from the GenomicRanges package has a fix argument that sets the anchor, so calling resize() coalesces anchoring and width modification. The coupling at the function call level is justified since the effect of setting the width depends on the anchor. However, plyranges increases cohesion and decouples the anchoring into its own function call. Increasing cohesion simplifies the interface to each operation, makes the meaning of arguments more intuitive, and relies on function names as the primary means of expression, instead of a more complex mixture of function and argument names. This results in the user being able to conceptualize the plyranges DSL as a flat catalog of functions, without having to descend further into documentation to understand a function’s arguments. A flat function catalog also enhances API discoverability, particularly through auto-completion in integrated developer environments (IDEs). One downside of pushing cohesion to this extreme is that function calls become coupled, and care is necessary to treat them as a group when modifying code. Like dplyr, plyranges verbs are functional: they are free of side effects and are generally endomorphic, meaning that when the input is a GRanges object they return a GRanges object. This enables chaining of verbs through syntax like the forward pipe operator from the magrittr package. This syntax has a direct cognitive mapping to natural language and the intuitive notion of pipelines. The low-level object-oriented APIs of Bioconductor tend to manipulate data via sub-replacement functions, like start(gr) &lt;- x. These ultimately produce the side effect of replacing a symbol mapping in the current environment and thus are not amenable to so-called fluent syntax. Expressiveness relates to the information content in code: the programmer should be able to clarify intent without unnecessary verbosity. For example, our overlap-based join operations are more concise than the multiple steps necessary to achieve the same effect in the original GenomicRanges API. In other cases, the plyranges API increases verbosity for the sake of clarity and cohesion. Explicitly calling anchor() can require more typing, but the code is easier to comprehend. Another example is the set of routines for importing genomic annotations, including read_gff(), read_bed(), and read_bam(). Compared to the generic import() in rtracklayer, the explicit format-based naming in plyranges clarifies intent and the type of data being returned. Similarly, every plyranges function that computes with strand information indicates its intentions by including suffixes such as directed, upstream or downstream in its name, otherwise strand is ignored. The GenomicRanges API does not make this distinction explicit in its function naming, instead relying on a parameter that defaults to strand sensitivity, an arguably confusing behavior. The implementation of plyranges is built on top of Bioconductor infrastructure, meaning most functions are constructed by composing generic functions from core Bioconductor packages. As a result, any Bioconductor packages that uses data structures that inherit from GRanges will be able to use plyranges for free. Another consequence of building on top of Bioconductor generics is that the speed and memory usage of plyranges functions are similar to the highly optimized methods implemented in Bioconductor for GRanges objects. A caveat to constructing a compatible interface with dplyr is that plyranges makes extensive use of non-standard evaluation in R via the rlang package (Henry and Wickham 2017). Simply, this means that computations are evaluated in the context of the GRanges objects. Both dplyr and plyranges are based on the rlang language, because it allows for more expressive code that is free of repeated references to the container. Implicitly referencing the container is particularly convenient when programming interactively. Consequently, when programming with plyranges, a user needs to generally understand the rlang language and how to adapt their code accordingly. Users familiar with the tidyverse should already have such knowledge. "],
["2-4-conclusion.html", "2.4 Conclusion", " 2.4 Conclusion We have shown how to create expressive and reproducible genomic workflows using the plyranges DSL. By realising that the GRanges data model is tidy we have highlighted how to implement a grammar for performing genomic arithmetic, aggregation, restriction and merging. Our examples show that plyranges code is succinct, human readable and can take advantage of the interoperability provided by the Bioconductor ecosystem and the R language. We also note that the grammar elements and design principles we have described are programming language agnostic and could be easily be implemented in another language where genomic information could be represented as a tabular data structure. We chose R because it is what we are familiar with and because the aforementioned Bioconductor packages have implemented the GRanges data structure. We aim to continue developing the plyranges package and to extend it for use with more complex data structures, such as the SummarizedExperiment class, the core Bioconductor data structure for representing experimental results (e.g., counts) from multiple sample experiments in conjunction with feature and sample metadata. Although, the SummarizedExperiment is not strictly tidy, it does consist of three tidy data structures that are related by feature and sample identifiers. Therefore, the grammar and design of the plryanges DSL is naturally extensible to the SummarizedExperiment. As the plyranges interface encourages tidy data practices, it integrates well with the grammar of graphics (Wickham, Hadley 2016). To achieve responsive performance, interactive graphics rely on lazy data access and computing patterns, so the deferred mechanisms within plyranges should help support interactive genomics applications. "],
["2-5-availability-of-data-and-materials.html", "2.5 Availability of Data and Materials", " 2.5 Availability of Data and Materials The BigWig file for the H3K27Me3 primary T CD8+ memory cells from peripheral blood ChIP-seq data from the Human Roadmap Epigenomics project was downloaded from the AnnotationHub package (2.13.1) under accession AH33458 (Morgan 2017; Roadmap Epigenomics Consortium et al. 2015). The BAM file corresponding to the H1 cell line ChIP-seq data is available at NCBI GEO under accession GSM433167 (Barrett et al. 2013; Roadmap Epigenomics Consortium et al. 2015). The SNP array data for the H1 cell line data is available at NCBI GEO under accession GPL18952 (Roadmap Epigenomics Consortium et al. 2015). The plyranges package is open source under an Artistic 2.0 license (Lee, Lawrence, and Cook 2018). The software can be obtained via the Bioconductor project website https://bioconductor.org or accessed via Github https://github.com/sa-lee/plyranges. "],
["acknowledgements-1.html", "Acknowledgements", " Acknowledgements We would like to thank Dr Matthew Ritchie at the Walter and Eliza Hall Institute and Dr Paul Harrison for their feedback on earlier drafts of this work. We would also like to thank Lori Shepherd and Hèrve Pages for the code review they performed and users who have submitted feedback and pull requests. "],
["3-ch-fluentGenomics.html", "Chapter 3 Fluent genomics with plyranges and tximeta", " Chapter 3 Fluent genomics with plyranges and tximeta We construct a simple workflow for fluent genomics data analysis using the R/Bioconductor ecosystem. This involves three core steps: import the data into an appropriate abstraction, model the data with respect to the biological questions of interest, and integrate the results with respect to their underlying genomic coordinates. Here we show how to implement these steps to integrate published RNA-seq and ATAC-seq experiments on macrophage cell lines. Using tximeta, we import RNA-seq transcript quantifications into an analysis-ready data structure, called the SummarizedExperiment, that contains the ranges of the reference transcripts and metadata on their provenance. Using SummarizedExperiments to represent the ATAC-seq and RNA-seq data, we model differentially accessible (DA) chromatin peaks and differentially expressed (DE) genes with existing Bioconductor packages. Using plyranges we then integrate the results to see if there is an enrichment of DA peaks near DE genes by finding overlaps and aggregating over log-fold change thresholds. The combination of these packages and their integration with the Bioconductor ecosystem provide a coherent framework for analysts to iteratively and reproducibly explore their biological data. "],
["3-1-introduction.html", "3.1 Introduction", " 3.1 Introduction In this workflow, we examine a subset of the RNA-seq and ATAC-seq data from Alasoo et al. (2018), a study that involved treatment of macrophage cell lines from a number of human donors with interferon gamma (IFNg), Salmonella infection, or both treatments combined. Alasoo et al. (2018) examined gene expression and chromatin accessibility in a subset of 86 successfully differentiated induced pluripotent stem cells (iPSC) lines, and compared baseline and response with respect to chromatin accessibility and gene expression at specific quantitative trait loci (QTL). The authors found that many of the stimulus-specific expression QTL were already detectable as chromatin QTL in naive cells, and further hypothesize about the nature and role of transcription factors implicated in the response to stimulus. We will perform a much simpler analysis than the one found in Alasoo et al. (2018), using their publicly available RNA-seq and ATAC-seq data (ignoring the genotypes). We will examine the effect of IFNg stimulation on gene expression and chromatin accessibility, and look to see if there is an enrichment of differentially accessible (DA) ATAC-seq peaks in the vicinity of differentially expressed (DE) genes. This is plausible, as the transcriptomic response to IFNg stimulation may be mediated through binding of regulatory proteins to accessible regions, and this binding may increase the accessibility of those regions such that it can be detected by ATAC-seq. Figure 3.1: An overview of the fluent genomics workflow. First, we import data as a SummarizedExperiment object, which enables interoperability with downstream analysis packages. Then we model our assay data, using the existing Bioconductor packages DESeq2 and limma. We take the results of our models for each assay with respect to their genomic coordinates, and integrate them. First, we compute the overlap between the results of each assay, then aggregate over the combined genomic regions, and finally summarize to compare enrichment for differentially expressed genes to non differentially expressed genes. The final output can be used for downstream visualization or further transformation. Throughout the workflow (Figure 3.1), we will use existing Bioconductor infrastructure to understand these datasets. In particular, we will emphasize the use of the Bioconductor packages plyranges and tximeta. The plyranges package fluently transforms data tied to genomic ranges using operations like shifting, window construction, overlap detection, etc. It is described by Lee, Cook, and Lawrence (2019) and leverages underlying core Bioconductor infrastructure (Lawrence et al. 2013b; Huber et al. 2015b) and the tidyverse design principles Wickham et al. (2019). The tximeta package described by Love et al. (2019) is used to read RNA-seq quantification data into R/Bioconductor, such that the transcript ranges and their provenance are automatically attached to the object containing expression values and differential expression results. 3.1.1 Experimental Data The data used in this workflow is available from two packages: the macrophage Bioconductor ExperimentData package and from the workflow package fluentGenomics. The macrophage package contains RNA-seq quantification from 24 RNA-seq samples, a subset of the RNA-seq samples generated and analyzed by Alasoo et al. (2018). The paired-end reads were quantified using Salmon (Patro et al. 2017), using the Gencode 29 human reference transcripts (Frankish, GENCODE-consoritum, and Flicek 2018). For more details on quantification, and the exact code used, consult the vignette of the macrophage package. The package also contains the Snakemake file that was used to distribute the Salmon quantification jobs on a cluster (Köster and Rahmann 2012). The fluentGenomics package contains functionality to download and generate a cached SummarizedExperiment object from the normalized ATAC-seq data provided by Alasoo and Gaffney (2017). This object contains all 145 ATAC-seq samples across all experimental conditions as analyzed by Alasoo et al. (2018). The data can be also be downloaded directly from the Zenodo deposition. The following code loads the path to the cached data file, or if it is not present, will create the cache and generate a SummarizedExperiment using the the BiocFileCache package (Shepherd and Morgan 2019). library(fluentGenomics) path_to_se &lt;- cache_atac_se() We can then read the cached file and assign it to an object called atac. atac &lt;- readRDS(path_to_se) A precise description of how we obtained this SummarizedExperiment object can be found in section 3.2.2. "],
["3-2-se.html", "3.2 Import Data as a SummarizedExperiment", " 3.2 Import Data as a SummarizedExperiment 3.2.1 Using tximeta to import RNA-seq quantification data First, we specify a directory dir, where the quantification files are stored. You could simply specify this directory with: dir &lt;- &quot;/path/to/quant/files&quot; where the path is relative to your current R session. However, in this case we have distributed the files in the macrophage package. The relevant directory and associated files can be located using system.file. dir &lt;- system.file(&quot;extdata&quot;, package=&quot;macrophage&quot;) Information about the experiment is contained in the coldata.csv file. We leverage the dplyr and readr packages (as part of the tidyverse) to read this file into R (Wickham et al. 2019). We will see later that plyranges extends these packages to accommodate genomic ranges. library(readr) library(dplyr) colfile &lt;- file.path(dir, &quot;coldata.csv&quot;) coldata &lt;- read_csv(colfile) %&gt;% dplyr::select( names, id = sample_id, line = line_id, condition = condition_name ) %&gt;% dplyr::mutate( files = file.path(dir, &quot;quants&quot;, names, &quot;quant.sf.gz&quot;), line = factor(line), condition = relevel(factor(condition), &quot;naive&quot;) ) coldata #&gt; # A tibble: 24 x 5 #&gt; names id line condition files #&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; #&gt; 1 SAMEA1038… diku_A diku… naive /Library/Frameworks/R.framework/Versions/… #&gt; 2 SAMEA1038… diku_B diku… IFNg /Library/Frameworks/R.framework/Versions/… #&gt; 3 SAMEA1038… diku_C diku… SL1344 /Library/Frameworks/R.framework/Versions/… #&gt; 4 SAMEA1038… diku_D diku… IFNg_SL13… /Library/Frameworks/R.framework/Versions/… #&gt; 5 SAMEA1038… eiwy_A eiwy… naive /Library/Frameworks/R.framework/Versions/… #&gt; 6 SAMEA1038… eiwy_B eiwy… IFNg /Library/Frameworks/R.framework/Versions/… #&gt; 7 SAMEA1038… eiwy_C eiwy… SL1344 /Library/Frameworks/R.framework/Versions/… #&gt; 8 SAMEA1038… eiwy_D eiwy… IFNg_SL13… /Library/Frameworks/R.framework/Versions/… #&gt; 9 SAMEA1038… fikt_A fikt… naive /Library/Frameworks/R.framework/Versions/… #&gt; 10 SAMEA1038… fikt_B fikt… IFNg /Library/Frameworks/R.framework/Versions/… #&gt; # … with 14 more rows After we have read the coldata.csv file, we select relevant columns from this table, create a new column called files, and transform the existing line and condition columns into factors. In the case of condition, we specify the “naive” cell line as the reference level. The files column points to the quantifications for each observation – these files have been gzipped, but would typically not have the ‘gz’ ending if used from Salmon directly. One other thing to note is the use of the pipe operator,%&gt;%, which can be read as “then”, i.e. first read the data, then select columns, then mutate them. Now we have a table summarizing the experimental design and the locations of the quantifications. The following lines of code do a lot of work for the analyst: importing the RNA-seq quantification (dropping inferential replicates in this case), locating the relevant reference transcriptome, attaching the transcript ranges to the data, and fetching genome information. Inferential replicates are especially useful for performing transcript-level analysis, but here we will use a point estimate for the per-gene counts and perform gene-level analysis. The result is a SummarizedExperiment object. suppressPackageStartupMessages(library(SummarizedExperiment)) library(tximeta) se &lt;- tximeta(coldata, dropInfReps=TRUE) se #&gt; class: RangedSummarizedExperiment #&gt; dim: 205870 24 #&gt; metadata(6): tximetaInfo quantInfo ... txomeInfo txdbInfo #&gt; assays(3): counts abundance length #&gt; rownames(205870): ENST00000456328.2 ENST00000450305.2 ... #&gt; ENST00000387460.2 ENST00000387461.2 #&gt; rowData names(3): tx_id gene_id tx_name #&gt; colnames(24): SAMEA103885102 SAMEA103885347 ... SAMEA103885308 #&gt; SAMEA103884949 #&gt; colData names(4): names id line condition On a machine with a working internet connection, the above command works without any extra steps, as the tximeta function obtains any necessary metadata via FTP, unless it is already cached locally. The tximeta package can also be used without an internet connection, in this case the linked transcriptome can be created directly from a Salmon index and gtf. makeLinkedTxome( indexDir=file.path(dir, &quot;gencode.v29_salmon_0.12.0&quot;), source=&quot;Gencode&quot;, organism=&quot;Homo sapiens&quot;, release=&quot;29&quot;, genome=&quot;GRCh38&quot;, fasta=&quot;gencode.v29.transcripts.fa.gz&quot;, # ftp link to fasta file gtf=file.path(dir, &quot;gencode.v29.annotation.gtf.gz&quot;), # local version write=FALSE ) Because tximeta knows the correct reference transcriptome, we can ask tximeta to summarize the transcript-level data to the gene level using the methods of Soneson, Love, and Robinson (2015). gse &lt;- summarizeToGene(se) One final note is that the start of positive strand genes and the end of negative strand genes is now dictated by the genomic extent of the isoforms of the gene (so the start and end of the reduced GRanges). Another alternative would be to either operate on transcript abundance, and perform differential analysis on transcript (and so avoid defining the TSS of a set of isoforms), or to use gene-level summarized expression but to pick the most representative TSS based on isoform expression. 3.2.2 Importing ATAC-seq data as a SummarizedExperiment object The SummarizedExperiment object containing ATAC-seq peaks can be created from the following tab-delimited files from Alasoo and Gaffney (2017): The sample metadata: ATAC_sample_metadata.txt.gz (&lt;1M) The matrix of normalized read counts: ATAC_cqn_matrix.txt.gz (109M) The annotated peaks: ATAC_peak_metadata.txt.gz (5.6M) To begin, we read in the sample metadata, following similar steps to those we used to generate the coldata table for the RNA-seq experiment: atac_coldata &lt;- read_tsv(&quot;ATAC_sample_metadata.txt.gz&quot;) %&gt;% select( sample_id, donor, condition = condition_name ) %&gt;% mutate(condition = relevel(factor(condition), &quot;naive&quot;)) The ATAC-seq counts have already been normalized with cqn (Hansen, Irizarry, and Wu 2012) and log2 transformed. Loading the cqn-normalized matrix of log2 transformed read counts takes ~30 seconds and loads an object of ~370 Mb. We set the column names so that the first column contains the rownames of the matrix, and the remaining columns are the sample identities from the atac_coldata object. atac_mat &lt;- read_tsv( &quot;ATAC_cqn_matrix.txt.gz&quot;, skip = 1, col_names = c(&quot;rownames&quot;, atac_coldata[[&quot;sample_id&quot;]]) ) rownames &lt;- atac_mat[[&quot;rownames&quot;]] atac_mat &lt;- as.matrix(atac_mat[,-1]) rownames(atac_mat) &lt;- rownames We read in the peak metadata (locations in the genome), and convert it to a GRanges object. The as_granges() function automatically converts the data.frame into a GRanges object. From that result, we extract the peak_id column and set the genome information to the build “GRCh38”. We know this from the Zenodo entry. library(plyranges) peaks_df &lt;- read_tsv( &quot;ATAC_peak_metadata.txt.gz&quot;, col_types = c(&quot;cidciicdc&quot;) ) peaks_gr &lt;- peaks_df %&gt;% as_granges(seqnames = chr) %&gt;% select(peak_id=gene_id) %&gt;% set_genome_info(genome = &quot;GRCh38&quot;) Finally, we construct a SummarizedExperiment object. We place the matrix into the assays slot as a named list, the annotated peaks into the row-wise ranges slot, and the sample metadata into the column-wise data slot: atac &lt;- SummarizedExperiment( assays = list(cqndata=atac_mat), rowRanges = peaks_gr, colData = atac_coldata ) "],
["3-3-model-assays.html", "3.3 Model assays", " 3.3 Model assays 3.3.1 RNA-seq differential gene expression analysis We can easily run a differential expression analysis with DESeq2 using the following code chunks (Love, Huber, and Anders 2014). The design formula indicates that we want to control for the donor baselines (line) and test for differences in gene expression on the condition. For a more comprehensive discussion of DE workflows in Bioconductor see Love et al. (2016) and Law et al. (2018). library(DESeq2) dds &lt;- DESeqDataSet(gse, ~line + condition) # filter out lowly expressed genes # at least 10 counts in at least 6 samples keep &lt;- rowSums(counts(dds) &gt;= 10) &gt;= 6 dds &lt;- dds[keep,] The model is fit with the following line of code: dds &lt;- DESeq(dds) Below we set the contrast on the condition variable, indicating we are estimating the \\(\\log_2\\) fold change (LFC) of IFNg stimulated cell lines against naive cell lines. We are interested in LFC greater than 1 at a nominal false discovery rate (FDR) of 1%. res &lt;- results(dds, contrast=c(&quot;condition&quot;,&quot;IFNg&quot;,&quot;naive&quot;), lfcThreshold=1, alpha=0.01) To see the results of the expression analysis, we can generate a summary table and an MA plot: summary(res) #&gt; #&gt; out of 17806 with nonzero total read count #&gt; adjusted p-value &lt; 0.01 #&gt; LFC &gt; 1.00 (up) : 502, 2.8% #&gt; LFC &lt; -1.00 (down) : 247, 1.4% #&gt; outliers [1] : 0, 0% #&gt; low counts [2] : 0, 0% #&gt; (mean count &lt; 3) #&gt; [1] see &#39;cooksCutoff&#39; argument of ?results #&gt; [2] see &#39;independentFiltering&#39; argument of ?results DESeq2::plotMA(res, ylim=c(-10,10)) Figure 3.2: Visualization of DESeq2 results as an “MA plot”. Genes that have an adjusted p-value below 0.01 are colored red. We now output the results as a GRanges object, and due to the conventions of plyranges, we construct a new column called gene_id from the row names of the results. Each row now contains the genomic region (seqnames, start, end, strand) along with corresponding metadata columns (the gene_id and the results of the test). Note that tximeta has correctly identified the reference genome as “hg38”, and this has also been added to the GRanges along the results columns. This kind of book-keeping is vital once overlap operations are performed to ensure that plyranges is not comparing across incompatible genomes. suppressPackageStartupMessages(library(plyranges)) de_genes &lt;- results(dds, contrast=c(&quot;condition&quot;,&quot;IFNg&quot;,&quot;naive&quot;), lfcThreshold=1, format=&quot;GRanges&quot;) %&gt;% names_to_column(&quot;gene_id&quot;) de_genes #&gt; GRanges object with 17806 ranges and 7 metadata columns: #&gt; seqnames ranges strand | gene_id #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; #&gt; [1] chrX 100627109-100639991 - | ENSG00000000003.14 #&gt; [2] chr20 50934867-50958555 - | ENSG00000000419.12 #&gt; [3] chr1 169849631-169894267 - | ENSG00000000457.13 #&gt; [4] chr1 169662007-169854080 + | ENSG00000000460.16 #&gt; [5] chr1 27612064-27635277 - | ENSG00000000938.12 #&gt; ... ... ... ... . ... #&gt; [17802] chr10 84167228-84172093 - | ENSG00000285972.1 #&gt; [17803] chr6 63572012-63583587 + | ENSG00000285976.1 #&gt; [17804] chr16 57177349-57181390 + | ENSG00000285979.1 #&gt; [17805] chr8 103398658-103501895 - | ENSG00000285982.1 #&gt; [17806] chr10 12563151-12567351 + | ENSG00000285994.1 #&gt; baseMean log2FoldChange lfcSE #&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; #&gt; [1] 171.570646163445 -0.282245015065582 0.300571026277417 #&gt; [2] 967.751278980391 0.0391222756936352 0.0859707605047955 #&gt; [3] 682.432885098654 1.2846178585311 0.196906721741941 #&gt; [4] 262.963397841117 -1.47187616421189 0.218691645887265 #&gt; [5] 2660.10225731917 0.675478091290521 0.236053041372838 #&gt; ... ... ... ... #&gt; [17802] 10.0474624496157 0.548451844773876 0.444318686394084 #&gt; [17803] 4586.34616821518 -0.033929582570062 0.188004977365846 #&gt; [17804] 14.2965310090402 0.312347650582085 0.522699844356108 #&gt; [17805] 27.7629588245413 0.994518742790125 1.58237312176743 #&gt; [17806] 6.60408582708505 0.25399752352481 0.5957511892896 #&gt; stat pvalue padj #&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; #&gt; [1] 0 1 1 #&gt; [2] 0 1 1 #&gt; [3] 1.44544511235177 0.148332899695748 1 #&gt; [4] -2.15772377722715 0.0309493141635637 0.409727500369082 #&gt; [5] 0 1 1 #&gt; ... ... ... ... #&gt; [17802] 0 1 1 #&gt; [17803] 0 1 1 #&gt; [17804] 0 1 1 #&gt; [17805] 0 1 1 #&gt; [17806] 0 1 1 #&gt; ------- #&gt; seqinfo: 25 sequences (1 circular) from hg38 genome From this, we can restrict the results to those that meet our FDR threshold and select (and rename) the metadata columns we’re interested in: de_genes &lt;- de_genes %&gt;% filter(padj &lt; 0.01) %&gt;% dplyr::select( gene_id, de_log2FC = log2FoldChange, de_padj = padj ) We now wish to extract genes for which there is evidence that the LFC is not large. We perform this test by specifying an LFC threshold and an alternative hypothesis (altHypothesis) that the LFC is less than the threshold in absolute value. To visualize the result of this test, you can run results without format=\"GRanges\", and pass this object to plotMA as before. We label these genes as other_genes and later as “non-DE genes”, for comparison with our de_genes set. other_genes &lt;- results(dds, contrast=c(&quot;condition&quot;,&quot;IFNg&quot;,&quot;naive&quot;), lfcThreshold=1, altHypothesis=&quot;lessAbs&quot;, format=&quot;GRanges&quot;) %&gt;% filter(padj &lt; 0.01) %&gt;% names_to_column(&quot;gene_id&quot;) %&gt;% dplyr::select( gene_id, de_log2FC = log2FoldChange, de_padj = padj ) 3.3.2 ATAC-seq peak differential abundance analysis The following section describes the process we have used for generating a GRanges object of differential peaks from the ATAC-seq data in Alasoo et al. (2018). The code chunks for the remainder of this section are not run. For assessing differential accessibility, we run limma (Smyth 2004), and generate the a summary of LFCs and adjusted p-values for the peaks: library(limma) design &lt;- model.matrix(~donor + condition, colData(atac)) fit &lt;- lmFit(assay(atac), design) fit &lt;- eBayes(fit) idx &lt;- which(colnames(fit$coefficients) == &quot;conditionIFNg&quot;) tt &lt;- topTable(fit, coef=idx, sort.by=&quot;none&quot;, n=nrow(atac)) We now take the rowRanges of the SummarizedExperiment and attach the LFCs and adjusted p-values from limma, so that we can consider the overlap with differential expression. Note that we set the genome build to “hg38” and restyle the chromosome information to use the “UCSC” style (e.g. “chr1”, “chr2”, etc.). Again, we know the genome build from the Zenodo entry for the ATAC-seq data. atac_peaks &lt;- rowRanges(atac) %&gt;% remove_names() %&gt;% mutate( da_log2FC = tt$logFC, da_padj = tt$adj.P.Val ) %&gt;% set_genome_info(genome = &quot;hg38&quot;) seqlevelsStyle(atac_peaks) &lt;- &quot;UCSC&quot; The final GRanges object containing the DA peaks is included in the workflow package and can be loaded as follows: library(fluentGenomics) peaks #&gt; GRanges object with 296220 ranges and 3 metadata columns: #&gt; seqnames ranges strand | peak_id #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; #&gt; [1] chr1 9979-10668 * | ATAC_peak_1 #&gt; [2] chr1 10939-11473 * | ATAC_peak_2 #&gt; [3] chr1 15505-15729 * | ATAC_peak_3 #&gt; [4] chr1 21148-21481 * | ATAC_peak_4 #&gt; [5] chr1 21864-22067 * | ATAC_peak_5 #&gt; ... ... ... ... . ... #&gt; [296216] chrX 155896572-155896835 * | ATAC_peak_296216 #&gt; [296217] chrX 155958507-155958646 * | ATAC_peak_296217 #&gt; [296218] chrX 156016760-156016975 * | ATAC_peak_296218 #&gt; [296219] chrX 156028551-156029422 * | ATAC_peak_296219 #&gt; [296220] chrX 156030135-156030785 * | ATAC_peak_296220 #&gt; da_log2FC da_padj #&gt; &lt;numeric&gt; &lt;numeric&gt; #&gt; [1] 0.266185396736073 9.10672732956434e-05 #&gt; [2] 0.32217712436691 2.03434717570469e-05 #&gt; [3] -0.574159538548115 3.41707743345703e-08 #&gt; [4] -1.14706617895329 8.22298606986521e-26 #&gt; [5] -0.896143162633654 4.79452571676397e-11 #&gt; ... ... ... #&gt; [296216] -0.834628897017445 1.3354605397165e-11 #&gt; [296217] -0.147537281935847 0.313014754316915 #&gt; [296218] -0.609732301631964 3.62338775135558e-09 #&gt; [296219] -0.347678474957794 6.94823191242968e-06 #&gt; [296220] 0.492442459200901 7.07663984067763e-13 #&gt; ------- #&gt; seqinfo: 23 sequences from hg38 genome; no seqlengths "],
["3-4-integrate-ranges.html", "3.4 Integrate ranges", " 3.4 Integrate ranges 3.4.1 Finding overlaps with plyranges We have already used plyranges a number of times above, to filter, mutate, and select on GRanges objects, as well as ensuring the correct genome annotation and style has been used. The plyranges package provides a grammar for performing transformations of genomic data (Lee, Cook, and Lawrence 2019). Computations resulting from compositions of plyranges “verbs” are performed using underlying, highly optimized range operations in the GenomicRanges package (Lawrence et al. 2013b). For the overlap analysis, we filter the annotated peaks to have a nominal FDR bound of 1%. da_peaks &lt;- peaks %&gt;% filter(da_padj &lt; 0.01) We now have GRanges objects that contain DE genes, genes without strong signal of DE, and DA peaks. We are ready to answer the question: is there an enrichment of DA ATAC-seq peaks in the vicinity of DE genes compared to genes without sufficient DE signal? 3.4.2 Down sampling non-differentially expressed genes As plyranges is built on top of dplyr, it implements methods for many of its verbs for GRanges objects. Here we can use slice to randomly sample the rows of the other_genes. The sample.int function will generate random samples of size equal to the number of DE-genes from the number of rows in other_genes: size &lt;- length(de_genes) slice(other_genes, sample.int(plyranges::n(), size)) #&gt; GRanges object with 749 ranges and 3 metadata columns: #&gt; seqnames ranges strand | gene_id #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; #&gt; [1] chr19 39825350-39834201 - | ENSG00000105204.13 #&gt; [2] chr19 14408819-14419383 - | ENSG00000123136.14 #&gt; [3] chr14 23969874-24006408 + | ENSG00000187630.16 #&gt; [4] chr20 58651253-58679526 + | ENSG00000124222.22 #&gt; [5] chr16 82147798-82170226 - | ENSG00000135698.9 #&gt; ... ... ... ... . ... #&gt; [745] chr19 52269571-52296046 + | ENSG00000196214.10 #&gt; [746] chr16 89644431-89657845 - | ENSG00000131165.14 #&gt; [747] chr12 133037292-133063304 + | ENSG00000198040.10 #&gt; [748] chr5 40825262-40835335 - | ENSG00000145592.13 #&gt; [749] chr5 138352596-138437028 + | ENSG00000120733.13 #&gt; de_log2FC de_padj #&gt; &lt;numeric&gt; &lt;numeric&gt; #&gt; [1] -0.474265547309344 0.00143202691714562 #&gt; [2] -0.195565169666109 6.15122704276795e-08 #&gt; [3] -0.193857259904103 2.41289228667999e-10 #&gt; [4] 0.124808527260134 4.01451204966896e-20 #&gt; [5] -0.475374492775655 0.0049180188694544 #&gt; ... ... ... #&gt; [745] 0.121805772890165 6.91333933127851e-14 #&gt; [746] 0.21540812408334 7.0132754879051e-13 #&gt; [747] -0.0247115944890518 1.03112429766339e-13 #&gt; [748] -0.465654436226396 2.88370054622836e-06 #&gt; [749] -0.0962891356396259 8.36348429017276e-17 #&gt; ------- #&gt; seqinfo: 25 sequences (1 circular) from hg38 genome We can repeat this many times to create many samples via replicate. By replicating the sub-sampling multiple times, we minimize the variance on the enrichment statistics induced by the sampling process. # set a seed for the results set.seed(2019-08-02) boot_genes &lt;- replicate(10, slice(other_genes, sample.int(plyranges::n(), size)), simplify = FALSE) This creates a list of GRanges objects as a list, and we can bind these together using the bind_ranges function. This function creates a new column called “resample” on the result that identifies each of the input GRanges objects: boot_genes &lt;- bind_ranges(boot_genes, .id = &quot;resample&quot;) Similarly, we can then combine the boot_genes GRanges, with the DE GRanges object. As the resample column was not present on the DE GRanges object, this is given a missing value which we recode to a 0 using mutate() all_genes &lt;- bind_ranges( de=de_genes, not_de = boot_genes, .id=&quot;origin&quot; ) %&gt;% mutate( origin = factor(origin, c(&quot;not_de&quot;, &quot;de&quot;)), resample = ifelse(is.na(resample), 0L, as.integer(resample)) ) all_genes #&gt; GRanges object with 8239 ranges and 5 metadata columns: #&gt; seqnames ranges strand | gene_id #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; #&gt; [1] chr1 196651878-196747504 + | ENSG00000000971.15 #&gt; [2] chr6 46129993-46146699 + | ENSG00000001561.6 #&gt; [3] chr4 17577192-17607972 + | ENSG00000002549.12 #&gt; [4] chr7 150800403-150805120 + | ENSG00000002933.8 #&gt; [5] chr4 15778275-15853230 + | ENSG00000004468.12 #&gt; ... ... ... ... . ... #&gt; [8235] chr17 43527844-43579620 - | ENSG00000175832.12 #&gt; [8236] chr17 18260534-18266552 + | ENSG00000177427.12 #&gt; [8237] chr20 63895182-63936031 + | ENSG00000101152.10 #&gt; [8238] chr1 39081316-39487177 + | ENSG00000127603.25 #&gt; [8239] chr8 41577187-41625001 + | ENSG00000158669.11 #&gt; de_log2FC de_padj resample origin #&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;integer&gt; &lt;factor&gt; #&gt; [1] 4.98711071930695 1.37057050625117e-13 0 de #&gt; [2] 1.92721595378787 3.1747750217733e-05 0 de #&gt; [3] 2.93372501059128 2.0131038573066e-11 0 de #&gt; [4] 3.16721751137972 1.07359906028984e-08 0 de #&gt; [5] 5.40894352968188 4.82904694023763e-18 0 de #&gt; ... ... ... ... ... #&gt; [8235] -0.240918426099239 0.00991611085813261 10 not_de #&gt; [8236] -0.166059030395757 9.1205141062356e-05 10 not_de #&gt; [8237] 0.250538999517482 1.74084544559733e-09 10 not_de #&gt; [8238] -0.385053503003028 0.00265539384929076 10 not_de #&gt; [8239] 0.155922038318879 2.9637514745875e-17 10 not_de #&gt; ------- #&gt; seqinfo: 25 sequences (1 circular) from hg38 genome 3.4.3 Expanding genomic coordinates around the transcription start site Now we would like to modify our gene ranges so they contain the 10 kilobases on either side of their transcription start site (TSS). There are many ways one could do this, but we prefer an approach via the anchoring methods in plyranges. Because there is a mutual dependence between the start, end, width, and strand of a GRanges object, we define anchors to fix one of start and end, while modifying the width. As an example, to extract just the TSS, we can anchor by the 5’ end of the range and modify the width of the range to equal 1. all_genes &lt;- all_genes %&gt;% anchor_5p() %&gt;% mutate(width = 1) Anchoring by the 5’ end of a range will fix the end of negatively stranded ranges, and fix the start of positively stranded ranges. We can then repeat the same pattern but this time using anchor_center() to tell plyranges that we are making the TSS the midpoint of a range that has total width of 20kb, or 10kb both upstream and downstream of the TSS. all_genes &lt;- all_genes %&gt;% anchor_center() %&gt;% mutate(width=2*1e4) 3.4.4 Use overlap joins to find relative enrichment We are now ready to compute overlaps between RNA-seq genes (our DE set and bootstrap sets) and the ATAC-seq peaks. In plyranges, overlaps are defined as joins between two GRanges objects: a left and a right GRanges object. In an overlap join, a match is any range on the left GRanges that is overlapped by the right GRanges. One powerful aspect of the overlap joins is that the result maintains all (metadata) columns from each of the left and right ranges which makes downstream summaries easy to compute. To combine the DE genes with the DA peaks, we perform a left overlap join. This returns to us the all_genes ranges (potentially with duplication), but with the metadata columns from those overlapping DA peaks. For any gene that has no overlaps, the DA peak columns will have NA’s. #&gt; GRanges object with 27766 ranges and 8 metadata columns: #&gt; seqnames ranges strand | gene_id #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; #&gt; [1] chr1 196641878-196661877 + | ENSG00000000971.15 #&gt; [2] chr6 46119993-46139992 + | ENSG00000001561.6 #&gt; [3] chr4 17567192-17587191 + | ENSG00000002549.12 #&gt; [4] chr4 17567192-17587191 + | ENSG00000002549.12 #&gt; [5] chr4 17567192-17587191 + | ENSG00000002549.12 #&gt; ... ... ... ... . ... #&gt; [27762] chr1 39071316-39091315 + | ENSG00000127603.25 #&gt; [27763] chr1 39071316-39091315 + | ENSG00000127603.25 #&gt; [27764] chr8 41567187-41587186 + | ENSG00000158669.11 #&gt; [27765] chr8 41567187-41587186 + | ENSG00000158669.11 #&gt; [27766] chr8 41567187-41587186 + | ENSG00000158669.11 #&gt; de_log2FC de_padj resample origin #&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;integer&gt; &lt;factor&gt; #&gt; [1] 4.98711071930695 1.37057050625117e-13 0 de #&gt; [2] 1.92721595378787 3.1747750217733e-05 0 de #&gt; [3] 2.93372501059128 2.0131038573066e-11 0 de #&gt; [4] 2.93372501059128 2.0131038573066e-11 0 de #&gt; [5] 2.93372501059128 2.0131038573066e-11 0 de #&gt; ... ... ... ... ... #&gt; [27762] -0.385053503003028 0.00265539384929076 10 not_de #&gt; [27763] -0.385053503003028 0.00265539384929076 10 not_de #&gt; [27764] 0.155922038318879 2.9637514745875e-17 10 not_de #&gt; [27765] 0.155922038318879 2.9637514745875e-17 10 not_de #&gt; [27766] 0.155922038318879 2.9637514745875e-17 10 not_de #&gt; peak_id da_log2FC da_padj #&gt; &lt;character&gt; &lt;numeric&gt; &lt;numeric&gt; #&gt; [1] ATAC_peak_21236 -0.546582189082724 0.000115273676444232 #&gt; [2] ATAC_peak_231183 1.45329684862127 9.7322474682763e-17 #&gt; [3] ATAC_peak_193578 0.222371496904895 3.00939005719989e-11 #&gt; [4] ATAC_peak_193579 -0.281615137872819 7.99888515457195e-05 #&gt; [5] ATAC_peak_193580 0.673705317951604 7.60042918890061e-15 #&gt; ... ... ... ... #&gt; [27762] ATAC_peak_5357 -1.05823584693303 3.69051674661467e-16 #&gt; [27763] ATAC_peak_5358 -1.31411238041643 6.44280493172654e-26 #&gt; [27764] ATAC_peak_263396 -0.904080135059089 8.19576651692093e-13 #&gt; [27765] ATAC_peak_263397 0.364737985368599 2.08834835864614e-08 #&gt; [27766] ATAC_peak_263399 0.317386691052334 1.20088116314111e-08 #&gt; ------- #&gt; seqinfo: 25 sequences (1 circular) from hg38 genome Now we can ask, how many DA peaks are near DE genes relative to “other” non-DE genes? A gene may appear more than once in genes_olap_peaks, because multiple peaks may overlap a single gene, or because we have re-sampled the same gene more than once, or a combination of these two cases. For each gene (that is the combination of chromosome, the start, end, and strand), and the “origin” (DE vs not-DE) we can compute the distinct number of peaks for each gene and the maximum peak based on LFC. This is achieved via reduce_ranges_directed, which allows an aggregation to result in a GRanges object via merging neighboring genomic regions. The use of the directed suffix indicates we’re maintaining strand information. In this case, we are simply merging ranges (genes) via the groups we mentioned above. We also have to account for the number of resamples we have performed when counting if there are any peaks, to ensure we do not double count the same peak: We can then filter genes if they have any peaks and compare the peak fold changes between non-DE and DE genes using a boxplot: library(ggplot2) gene_peak_max_lfc %&gt;% filter(peak_count &gt; 0) %&gt;% as.data.frame() %&gt;% ggplot(aes(origin, peak_max_lfc)) + geom_boxplot() Figure 3.3: A boxplot of maximum LFCs for DA peaks for DE genes compared to non-DE genes where genes have at least one DA peak. In general, the DE genes have larger maximum DA fold changes relative to the non-DE genes. Next we examine how thresholds on the DA LFC modify the enrichment we observe of DA peaks near DE or non-DE genes. First, we want to know how the number of peaks within DE genes and non-DE genes change as we change threshold values on the peak LFC. As an example, we could compute this by arbitrarily chosen LFC thresholds of 1 or 2 as follows: origin_peak_lfc &lt;- genes_olap_peaks %&gt;% group_by(origin) %&gt;% summarize( peak_count = sum(!is.na(da_padj)) / plyranges::n_distinct(resample), lfc1_peak_count =sum(abs(da_log2FC) &gt; 1, na.rm=TRUE)/ plyranges::n_distinct(resample), lfc2_peak_count = sum(abs(da_log2FC) &gt; 2, na.rm=TRUE)/ plyranges::n_distinct(resample) ) origin_peak_lfc #&gt; DataFrame with 2 rows and 4 columns #&gt; origin peak_count lfc1_peak_count lfc2_peak_count #&gt; &lt;factor&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; #&gt; 1 not_de 2391.8 369.5 32.5 #&gt; 2 de 3416 1097 234 Here we see that DE genes tend to have more DA peaks near them, and that the number of DA peaks decreases as we increase the DA LFC threshold (as expected). We now show how to compute the ratio of peak counts from DE compared to non-DE genes, so we can see how this ratio changes for various DA LFC thresholds. For all variables except for the origin column we divide the first row’s values by the second row, which will be the enrichment of peaks in DE genes compared to other genes. This requires us to reshape the summary table from long form back to wide form using the tidyr package. First we pivot the results of the peak_count columns into name-value pairs, then pivot again to place values into the origin column. Then we create a new column with the relative enrichment: origin_peak_lfc %&gt;% as.data.frame() %&gt;% tidyr::pivot_longer(cols = -origin) %&gt;% tidyr::pivot_wider(names_from = origin, values_from = value) %&gt;% mutate(enrichment = de / not_de) #&gt; # A tibble: 3 x 4 #&gt; name not_de de enrichment #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 peak_count 2392. 3416 1.43 #&gt; 2 lfc1_peak_count 370. 1097 2.97 #&gt; 3 lfc2_peak_count 32.5 234 7.2 The above table shows that relative enrichment increases for a larger LFC threshold. Due to the one-to-many mappings of genes to peaks, it is unknown if we have the same number of DE genes participating or less, as we increase the threshold on the DA LFC. We can examine the number of genes with overlapping DA peaks at various thresholds by grouping and aggregating twice. First, the number of peaks that meet the thresholds are computed within each gene, origin, and resample group. Second, within the origin column, we compute the total number of peaks that meet the DA LFC threshold and the number of genes that have more than zero peaks (again averaging over the number of resamples). genes_olap_peaks %&gt;% group_by(gene_id, origin, resample) %&gt;% reduce_ranges_directed( lfc1 = sum(abs(da_log2FC) &gt; 1, na.rm=TRUE), lfc2 = sum(abs(da_log2FC) &gt; 2, na.rm=TRUE) ) %&gt;% group_by(origin) %&gt;% summarize( lfc1_gene_count = sum(lfc1 &gt; 0) / plyranges::n_distinct(resample), lfc1_peak_count = sum(lfc1) / plyranges::n_distinct(resample), lfc2_gene_count = sum(lfc2 &gt; 0) / plyranges::n_distinct(resample), lfc2_peak_count = sum(lfc2) / plyranges::n_distinct(resample) ) #&gt; DataFrame with 2 rows and 5 columns #&gt; origin lfc1_gene_count lfc1_peak_count lfc2_gene_count lfc2_peak_count #&gt; &lt;factor&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; #&gt; 1 not_de 271.2 369.5 30.3 32.5 #&gt; 2 de 515 1097 151 234 To do this for many thresholds is cumbersome and would create a lot of duplicate code. Instead we create a single function called count_above_threshold that accepts a variable and a vector of thresholds, and computes the sum of the absolute value of the variable for each element in the thresholds vector. count_if_above_threshold &lt;- function(var, thresholds) { lapply(thresholds, function(.) sum(abs(var) &gt; ., na.rm = TRUE)) } The above function will compute the counts for any arbitrary threshold, so we can apply it over possible LFC thresholds of interest. We choose a grid of one hundred thresholds based on the range of absolute LFC values in the da_peaks GRanges object: thresholds &lt;- da_peaks %&gt;% mutate(abs_lfc = abs(da_log2FC)) %&gt;% with( seq(min(abs_lfc), max(abs_lfc), length.out = 100) ) The peak counts for each threshold are computed as a new list-column called value. First, the GRanges object has been grouped by the gene, origin, and the number of resamples columns. Then we aggregate over those columns, so each row will contain the peak counts for all of the thresholds for a gene, origin, and resample. We also maintain another list-column that contains the threshold values. genes_peak_all_thresholds &lt;- genes_olap_peaks %&gt;% group_by(gene_id, origin, resample) %&gt;% reduce_ranges_directed( value = count_if_above_threshold(da_log2FC, thresholds), threshold = list(thresholds) ) genes_peak_all_thresholds #&gt; GRanges object with 8239 ranges and 5 metadata columns: #&gt; seqnames ranges strand | gene_id origin #&gt; &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; &lt;factor&gt; #&gt; [1] chr1 196641878-196661877 + | ENSG00000000971.15 de #&gt; [2] chr6 46119993-46139992 + | ENSG00000001561.6 de #&gt; [3] chr4 17567192-17587191 + | ENSG00000002549.12 de #&gt; [4] chr7 150790403-150810402 + | ENSG00000002933.8 de #&gt; [5] chr4 15768275-15788274 + | ENSG00000004468.12 de #&gt; ... ... ... ... . ... ... #&gt; [8235] chr17 43569620-43589619 - | ENSG00000175832.12 not_de #&gt; [8236] chr17 18250534-18270533 + | ENSG00000177427.12 not_de #&gt; [8237] chr20 63885182-63905181 + | ENSG00000101152.10 not_de #&gt; [8238] chr1 39071316-39091315 + | ENSG00000127603.25 not_de #&gt; [8239] chr8 41567187-41587186 + | ENSG00000158669.11 not_de #&gt; resample value #&gt; &lt;integer&gt; &lt;IntegerList&gt; #&gt; [1] 0 1,1,1,... #&gt; [2] 0 1,1,1,... #&gt; [3] 0 6,6,6,... #&gt; [4] 0 4,4,4,... #&gt; [5] 0 11,11,11,... #&gt; ... ... ... #&gt; [8235] 10 1,1,1,... #&gt; [8236] 10 3,3,2,... #&gt; [8237] 10 5,5,5,... #&gt; [8238] 10 3,3,3,... #&gt; [8239] 10 3,3,3,... #&gt; threshold #&gt; &lt;NumericList&gt; #&gt; [1] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; [2] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; [3] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; [4] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; [5] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; ... ... #&gt; [8235] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; [8236] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; [8237] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; [8238] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; [8239] 0.0658243106359027,0.118483961449043,0.171143612262182,... #&gt; ------- #&gt; seqinfo: 25 sequences (1 circular) from hg38 genome Now we can expand these list-columns into a long GRanges object using the expand_ranges() function. This function will unlist the value and threshold columns and lengthen the resulting GRanges object. To compute the peak and gene counts for each threshold, we apply the same summarization as before: origin_peak_all_thresholds &lt;- genes_peak_all_thresholds %&gt;% expand_ranges() %&gt;% group_by(origin, threshold) %&gt;% summarize( gene_count = sum(value &gt; 0) / plyranges::n_distinct(resample), peak_count = sum(value) / plyranges::n_distinct(resample) ) origin_peak_all_thresholds #&gt; DataFrame with 200 rows and 4 columns #&gt; origin threshold gene_count peak_count #&gt; &lt;factor&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; #&gt; 1 not_de 0.0658243106359027 708 2391.4 #&gt; 2 not_de 0.118483961449043 698.8 2320.6 #&gt; 3 not_de 0.171143612262182 686.2 2178.6 #&gt; 4 not_de 0.223803263075322 672.4 1989.4 #&gt; 5 not_de 0.276462913888462 650.4 1785.8 #&gt; ... ... ... ... ... #&gt; 196 de 5.06849113788419 2 2 #&gt; 197 de 5.12115078869733 0 0 #&gt; 198 de 5.17381043951047 0 0 #&gt; 199 de 5.22647009032361 0 0 #&gt; 200 de 5.27912974113675 0 0 Again we can compute the relative enrichment in LFCs in the same manner as before, by pivoting the results to long form then back to wide form to compute the enrichment. We visualize the peak enrichment changes of DE genes relative to other genes as a line chart: origin_threshold_counts &lt;- origin_peak_all_thresholds %&gt;% as.data.frame() %&gt;% tidyr::pivot_longer(cols = -c(origin, threshold), names_to = c(&quot;type&quot;, &quot;var&quot;), names_sep = &quot;_&quot;, values_to = &quot;count&quot;) %&gt;% dplyr::select(-var) origin_threshold_counts %&gt;% filter(type == &quot;peak&quot;) %&gt;% tidyr::pivot_wider(names_from = origin, values_from = count) %&gt;% mutate(enrichment = de / not_de) %&gt;% ggplot(aes(x = threshold, y = enrichment)) + geom_line() + labs(x = &quot;logFC threshold&quot;, y = &quot;Relative Enrichment&quot;) Figure 3.4: A line chart displaying how relative enrichment of DA peaks change between DE genes compared to non-DE genes as the absolute DA LFC threshold increases. We computed the sum of DA peaks near the DE genes, for increasing LFC thresholds on the accessibility change. As we increased the threshold, the number of total peaks went down (likewise the mean number of DA peaks per gene). It is also likely the number of DE genes with a DA peak nearby with such a large change went down. We can investigate this with a plot that summarizes many of the aspects underlying the enrichment plot above. origin_threshold_counts %&gt;% ggplot(aes(x = threshold, y = count + 1, color = origin, linetype = type)) + geom_line() + scale_y_log10() Figure 3.5: A line chart displaying how gene and peak counts change as the absolute DA LFC threshold increases. Lines are colored according to whether they represent a gene that is DE or not. Note the x-axis is on a \\(log_{10}\\) scale. "],
["3-5-discussion-1.html", "3.5 Discussion", " 3.5 Discussion We have shown that by using plyranges and tximeta (with support of Bioconductor and tidyverse ecosystems) we can fluently iterate through the biological data science workflow: from import, through to modeling, and data integration. There are several further steps that would be interesting to perform in this analysis; for example, we could modify window size around the TSS to see how it affects enrichment, and vary the FDR cut-offs for both the DE gene and DA peak sets. We could also have computed variance in addition to the mean of the bootstrap set, and so drawn an interval around the enrichment line. Finally, our workflow illustrates the benefits of using appropriate data abstractions provided by Bioconductor such as the SummarizedExperiment and GRanges. These abstractions provide users with a mental model of their experimental data and are the building blocks for constructing the modular and iterative analyses we have shown here. Consequently, we have been able to interoperate many decoupled R packages (from both Bioconductor and the tidyverse) to construct a seamless end-to-end workflow that is far too specialized for a single monolithic tool. "],
["3-6-software-availability.html", "3.6 Software Availability", " 3.6 Software Availability The workflow materials can be fully reproduced following the instructions found at the Github repository sa-lee/fluentGenomics. Moreover, the development version of the workflow and all downstream dependencies can be installed using the BiocManager package by running: # development version from Github BiocManager::install(&quot;sa-lee/fluentGenomics&quot;) # version available from Bioconductor BiocManager::install(&quot;fluentGenomics&quot;) "],
["acknowledgements-2.html", "Acknowledgements", " Acknowledgements We would like to thank all participants of the Bioconductor 2019 and BiocAsia 2019 conferences who attended and provided feedback on early versions of this workflow paper. "],
["4-ch-intron.html", "Chapter 4 Tidy coverage analysis and visualisation with superintronic ", " Chapter 4 Tidy coverage analysis and visualisation with superintronic "],
["4-1-introduction-1.html", "4.1 Introduction", " 4.1 Introduction "],
["4-2-methods.html", "4.2 Methods", " 4.2 Methods 4.2.1 Motivation: intron signal in RNA-seq data "],
["4-3-results-1.html", "4.3 Results", " 4.3 Results 4.3.1 Representation of coverage estimation 4.3.2 Integration of external gene annotations 4.3.3 Discovery of regions of interest via ‘ranglers’ 4.3.4 A superintronic workflow for uncovering intron retention "],
["4-4-discussion-2.html", "4.4 Discussion", " 4.4 Discussion "],
["4-5-conclusion-1.html", "4.5 Conclusion", " 4.5 Conclusion "],
["5-ch-tsne.html", "Chapter 5 Casting multiple shadows: high-dimensional interactive data visualisation with tours and embeddings", " Chapter 5 Casting multiple shadows: high-dimensional interactive data visualisation with tours and embeddings There has been a rapid uptake in the use of non-linear dimensionality reduction (NLDR) methods such as t-distributed stochastic neighbour embedding (t-SNE) in the natural sciences as part of cluster orientation and dimension reduction workflows. The appropriate use of these methods is made difficult by their complex parameterisations and the multitude of decisions required to balance the preservation of local and global structure in the resulting visualisation. We present a visual analytics framework for the pragmatic usage of NLDR methods by combining them with a technique called the tour. A tour is a sequence of interpolated linear projections of multivariate data onto a lower dimensional space. The sequence is displayed as a dynamic visualisation, allowing a user to see the shadows the high-dimensional data casts in a lower dimensional view. By linking the tour to a view obtained from an NLDR method, we can preserve global structure and through user interactions like spatial linked brushing observe where the NLDR view may be misleading. We display several case studies from single cell genomics, that shows our approach is useful for cluster orientation tasks. The implementation of our framework is available as an R package called liminal available at https://github.com/sa-lee/liminal. "],
["5-1-introduction-2.html", "5.1 Introduction", " 5.1 Introduction High dimensional data is increasingly prevalent in the natural sciences and beyond but presents a challenge to the analyst in terms of both data cleaning / pre-processing and visualisation. Methods to embed data from a high-dimensional space into a low-dimensional one now form a core step of the data analysis workflow where they are used to ascertain hidden structure and de-noise data for downstream analysis (thereby nullifying the ‘curse of dimensionality’). Choosing an appropriate embedding presents a challenge to the analyst. How does an analyst know whether the embedding has captured the underlying topology and geometry of the high dimensional space? The answer depends on the analyst’s workflow. Brehmer et al. (2014) characterised two main workflow steps that an analyst performs when using embedding techniques: dimension reduction and cluster orientation. The first relates to dimension reduction achieved by using an embedding method, here an analyst wants to characterise and map meaning onto the embedded form, for example identifying batch effects from a high throughput sequencing experiment, or identifying a gradient or trajectory along the embedded form Nguyen and Holmes (2019). The second relates to using embeddings as part of a clustering workflow. Here analysts are interested in identifying and naming clusters and verifying them by either applying known labels or colouring by variables that are a-priori known to distinguish clusters. Both of these workflow steps rely on the embedding being ‘faithful’ or the original high dimensional dataset, and become much more difficult when there is no underlying ground truth. Embedding methods can be classified into two broad groups: linear and non-linear methods. Linear methods perform a linear transformation of the data; one example is principal components analysis (PCA) which performs an eigendecomposition of the estimated sample covariance matrix. The eigenvalues are sorted in decreasing order and represent the variance explained by each component (eigenvector). A common approach to deciding on the number of principal components to retain is to plot the proportion of variance explained by each component and choose a cut-off. Non-linear methods generally perform pre-processing on the high-dimensional data such as generating a neighborhood graph and perform transformations on the pre-processed form. We restrict our attention to three methods that are commonly used in high-throughput biology: t-distributed stochastic neighbor embedding (t-SNE), uniform manifold alignment and projection (UMAP), and potential of heat-diffusion for affinity-based transition embedding (PHATE). The t-SNE algorithm estimates the similarity of (Euclidean) distances of points in a high dimensional space using a Gaussian distribution and then estimates a configuration in the low dimensional embedding space by modelling similarities using a t-distribution with 1 degree of freedom. The resulting configuration is the one that minimizes the Kullback-Leibler divergence between the two distributions. A recent theoretical contribution by Linderman and Steinerberger (2019) proved that t-SNE can recover spherical and well separated cluster shapes, and proposed new approaches for tuning the optimisation parameters. It is a known problem that t-SNE can have trouble recovering global structure and that configurations can be highly dependent on how the algorithm is initialised and parameterized (Wattenberg, Viégas, and Johnson 2016; Kobak and Berens 2019). UMAP is a method that is related to LargeVis (Tang et al. 2016), and like t-SNE acts on the k-nearest neighbor graph. Its main differences are that it uses a different cost function (cross entropy) which is optimized using stochastic gradient descent and defines a different kernel for similarities in the low dimensional space. Due to it’s computational speed it’s possible to generate UMAP embeddings in more than three dimensions. Finally, PHATE, inspired by diffusion maps, is based on estimating an affinity matrix via a distance matrix and k-nearest neighbors graph. The algorithm de-noises estimated distances in high dimensional space via transforming the affinity matrix into a Markov transition probability matrix and diffusing this matrix over a fixed number of time steps. Then the diffused probabilities are transformed once more to construct a distance matrix, and multidimensional scaling is performed to construct a 2d embedding for visualization. As part of a visualization workflow, it’s important to consider the perception and interpretation of embedding methods as well. Sedlmair, Munzner, and Tory (2013) showed that 2D scatter plots were mostly sufficient for detecting class separation, however they noted that often multiple embeddings were required. For the task of cluster identification, Lewis, Van der Maaten, and Sa (2012) showed experimentally that novice users of non-linear embedding techniques were more likely to consider clusters of points on a 2d scatter plot to be the result of a spurious embedding compared to advanced users who were aware of the inner workings of the embedding algorithm. A complementary approach for visualizing structure in high dimensional data is the tour. A tour is a sequence of projections of a high dimensional dataset onto a low-dimensional orthonormal basis matrix, that is represented as a dynamic visualization. The sequence of generated bases are interpolated to form the tour path, allowing a user to explore the subspace of projections. A grand tour corresponds to choosing new bases at random, and can give an overview of the structure in the data. Instead of picking projections at random, a guided tour can be used to generate a sequence ‘interesting’ projections as quantified by an index function. Given the dynamic nature of the tour, user interaction is important for controlling and exploring the visualisation: the tour has been used previously by Wickham, Cook, and Hofmann (2015) as tool for exploring statistical model fits and by Buja, Cook, and Swayne (1996) for exploring factorial experimental designs. While there has been much work on the algorithmic details of the aforementioned embedding methods, there has been relatively few tools designed to assist users to interact with these techniques and assist them in making comparisons between embeddings and performing the aforementioned cluster orientation tasks. Several interactive interfaces have been proposed for evaluating or using embedding techniques: the Sleepwalk interface provides a click and highlight visualisation for colouring points in an embedding according to their distance in the original high-dimensional space (Ovchinnikova and Anders 2019). The work by Pezzotti et al. (2017) provides a user guided and modified form of the t-SNE algorithm, that allows users to modified optimisation parameters in real-time. Similarly, the embedding projector is a web interface to running UMAP, t-SNE or PCA live in the browser and provides interactions to color points, and highlights nearest neighbors (Smilkov et al. 2016). There is no one-size fits all: finding an appropriate embedding for a given dataset is a difficult and somewhat poorly defined problem. For non-linear methods, there are a lot of parameters to explore that can have an effect on the resulting visualisation and interpretation. Interfaces for evaluating embeddings require interaction but should also be able to be incorporated into an analysts workflow. We propose a more pragmatic workflow inspired by incorporating interactive graphics and tours with embeddings that allows users to see a global overview of their high dimensional data and assists them with cluster orientation tasks. This workflow is incorporated into an R package called liminal (Available: https://github.com/sa-lee/liminal). "],
["5-2-design.html", "5.2 Design", " 5.2 Design We propose using tours as part of an analyst’s workflow in in performing dimensionality reduction tasks. We have made extensive use of ensemble graphics, that is aligning related plots alongside each other to provide context. As we will see in the case studies, this allows analysts to quickly compare views from embedding methods and allows them to see how the embedding method alters the global structure of their data. Using ensembles allows the use of interaction techniques, that allow analysts to perform cluster orientation tasks via linking multiple views. This approach allows our interface, to achieve the three principles for interactive high-dimensional data visualisation outlined by Buja, Cook, and Swayne (1996): finding gestalt, posing queries, and making comparisons. 5.2.1 Finding Gestalt: focus and context To investigate latent structure and the shape of a high dimensional dataset, a tour can be run without the use of an external embedding. It is often useful to first run principal components on the input as an initial dimension reduction step, and then tour a subset of those components instead, i.e. by selecting them from a scree plot. The default tour layout is a scatter plot with an axis layout displaying the magnitude and direction of each basis vector. Since the tour is dynamic, it is often useful to be able to pause and highlight a particular view. In our interface, brushing will pause the tour path, allowing users to identify ‘interesting’ projections. The domain of the axis scales from running a tour is called the half range, and is computed by rescaling the input data onto hyper-dimensional unit cube. We bind the half range to a mouse wheel event, allowing a user to pan and zoom on the tour view dynamically. This is useful for peeling back dense clumps of points to reveal structure. 5.2.2 Posing Queries: multiple views, many contexts We have combined the tour view in a side by side layout with a scatter plot view that represents the output of an embedding algorithm. These views are linked; analysts can brush regions or highlight collections of points in either view. Linked highlighting can be performed when points have been previously labeled according to some discrete structure, i.e. cluster labels are available. This is achieved via the analyst clicking on groups in the legend, which causes unselected groupings to have their points become less opaque. Consequently, simple linked highlighting can alleviate a known downfall of methods such as UMAP or t-SNE: that is distances between clusters are misleading. By highlighting corresponding clusters in the tour view, the analyst can see the relationship between clusters, and therefore obtain a more accurate representation of the topology of their data. Simple linked brushing is achieved via mouse-click and drag movements. By default, when brushing occurs in the tour view, the current projection is paused and corresponding points in the embedding view are highlighted. Likewise, when brushing occurs in the embedding view, corresponding points in the tour view are highlighted. In this case, an analyst can use brushing for manually identifying clusters and verifying cluster locations and shapes: brushing in the embedding view gives analysts a sense of the shape and proximity of cluster in high-dimensional space. 5.2.3 Making comparisons: revising embeddings As mentioned previously, when using any DR method, we are assuming the embedding is representative of the high-dimensional dataset it was computed from. Defining what it means for embedding to be ‘representative` or ’faithful’ to high-dimensional data is ill-posed and depends on the underlying task an analyst is trying to achieve. At the very minimum, we are interested in distortions and diffusions of the high-dimensional data. Distortions occur when points that are near each other in the embedding view are far from each other in the original dataset. This implies that the embedding is not continuous. Diffusions occur when points are far from each other in the embedding view are near in the original data. Whether, points are near or far is reliant on the distance metric used; distortions and diffusions can be thought of as the preservation of distances or the nearest neighbors graphs between the high-dimensional space and the embedding space. As distances can be noisy in high-dimensions, ranks can be used instead as has been proposed by … Identifying distortions and diffusions allows an analyst to investigate the quality of their embedding and revise them iteratively. We propose this can be done visually using our side-by-side tour and embedding views. Some quality checks, such as finding distortions can be identified using simple linked brushing, however we can also interrogate them via spatial brushes and brush composition. Look up nearest neighbours graph from points that lie in a brushing region. Highlight the corresponding neighbours using colour or transparency in the linked view. The \\(k\\) nearest neighbours graph can be pre-computed quickly, for either \\(X\\), \\(Y\\) or both. Instead of using the neighbour indices, we could use the neighbour distances instead. Composition of multiple brushes could be used to show where there are matches/mismatches between nearest neighbour graphs. Using a linked neighbourhood brush, we can visually investigate the nearest neighbour relationships in the high-dimensional space via brushing in the embedding view. The user can select the number of nearest neighbours directly, and modify the distance metric used for determining the neighbours. This allows users to interrogate the stability of clusters generated in the embedding view. Multiple brushes can be used to pose queries in either the tour view or the embedding view; interface controls allow these brushes to combine using logical operators such as ‘and’, ‘or’, or ‘not’. The use of linked brushing goes beyond simple color highlights, allowing analysts to get a more holistic view of the effect of an embedding algorithm. "],
["5-3-implementation.html", "5.3 Implementation", " 5.3 Implementation We have implemented the above design as an open source R package called liminal (R Core Team 2019b). The package allows analysts to construct concatenated visualisations, via the Vega-Lite grammar of interactive graphics (using vegawidget package) and provides an interface for constructing brushes and manipulating tour paths using the shiny and tourr packages. liminal also provides a stand-alone interface to the tour, in addition to the linked scatterplot interfaces discussed above. Below we display the liminal API, and our approach to generating tour paths and user interactions. 5.3.1 Tours as a streaming data problem don’t need to realise the entire sequence instead generate new bases according to a fixed frame rate allows user interactions to play/pause on the current view 5.3.2 Linking views via brushes "],
["5-4-case-studies.html", "5.4 Case Studies", " 5.4 Case Studies 5.4.1 Case Study 1: Exploring tree structured data with tours and t-SNE Figure 5.1: Example from PHATE 5.4.1.1 Using t-SNE there’s a lot of parameters to tweak emphasises locality, distance between clusters of points can be misleading size of clusters can be misleading may require a few different runs to capture topology of data if there are clusters it will find them 5.4.2 Case Study 2: Clustering PBMC 10x single cell RNA-seq data 5.4.3 Case Study 3: Single cell mouse retina data "],
["5-5-discussion-3.html", "5.5 Discussion", " 5.5 Discussion "],
["5-6-acknowledgements-3.html", "5.6 Acknowledgements", " 5.6 Acknowledgements "],
["6-ch-conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion "],
["6-1-findings-and-limitations.html", "6.1 Findings and limitations", " 6.1 Findings and limitations "],
["6-2-software-development-and-impact.html", "6.2 Software development and impact", " 6.2 Software development and impact "],
["6-3-further-research.html", "6.3 Further research", " 6.3 Further research "],
["bibliography.html", "Bibliography", " Bibliography "]
]
